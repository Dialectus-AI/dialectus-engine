{
  "data": [
    {
      "id": "meta-llama/llama-3.1-405b-instruct:free",
      "name": "Meta: Llama 3.1 405B Instruct (free)",
      "provider": "openrouter",
      "description": "The highly anticipated 400B class of Llama3 is here! Clocking in at 128k context with impressive eval scores, the Meta AI team continues to push the frontier of open-source LLMs.\n\nMeta's latest class of model (Llama 3.1) launched with a variety of sizes & flavors. This 405B instruct-tuned version is optimized for high quality dialogue usecases.\n\nIt has demonstrated strong performance compared to leading closed-source models including GPT-4o and Claude 3.5 Sonnet in evaluations.\n\nTo read more about the model release, [click here](https://ai.meta.com/blog/meta-llama-3-1/). Usage of this model is subject to [Meta's Acceptable Use Policy](https://llama.meta.com/llama3/use-policy/).",
      "weight_class": "ultraweight",
      "tier": "flagship",
      "context_length": 65536,
      "max_completion_tokens": 4096,
      "pricing": {
        "prompt_cost_per_1k": 0.0,
        "completion_cost_per_1k": 0.0,
        "is_free": true,
        "currency": "USD"
      },
      "value_score": 16.896,
      "is_preview": false,
      "is_text_only": true,
      "estimated_params": "405B",
      "source_info": {
        "openrouter_raw": {
          "id": "meta-llama/llama-3.1-405b-instruct:free",
          "name": "Meta: Llama 3.1 405B Instruct (free)",
          "created": 1721692800,
          "description": "The highly anticipated 400B class of Llama3 is here! Clocking in at 128k context with impressive eval scores, the Meta AI team continues to push the frontier of open-source LLMs.\n\nMeta's latest class of model (Llama 3.1) launched with a variety of sizes & flavors. This 405B instruct-tuned version is optimized for high quality dialogue usecases.\n\nIt has demonstrated strong performance compared to leading closed-source models including GPT-4o and Claude 3.5 Sonnet in evaluations.\n\nTo read more about the model release, [click here](https://ai.meta.com/blog/meta-llama-3-1/). Usage of this model is subject to [Meta's Acceptable Use Policy](https://llama.meta.com/llama3/use-policy/).",
          "architecture": {
            "input_modalities": [
              "text"
            ],
            "output_modalities": [
              "text"
            ],
            "tokenizer": "Llama3",
            "instruct_type": "llama3"
          },
          "top_provider": {
            "is_moderated": false,
            "context_length": 65536,
            "max_completion_tokens": null
          },
          "pricing": {
            "prompt": "0",
            "completion": "0",
            "image": "0",
            "request": "0",
            "web_search": "0",
            "internal_reasoning": "0",
            "input_cache_read": "0",
            "input_cache_write": "0"
          },
          "canonical_slug": "meta-llama/llama-3.1-405b-instruct",
          "context_length": 65536,
          "hugging_face_id": "meta-llama/Meta-Llama-3.1-405B-Instruct",
          "per_request_limits": null,
          "supported_parameters": [
            "frequency_penalty",
            "max_tokens",
            "presence_penalty",
            "response_format",
            "stop",
            "structured_outputs",
            "temperature",
            "top_k",
            "top_p"
          ]
        }
      }
    },
    {
      "id": "openai/gpt-4o:extended",
      "name": "OpenAI: GPT-4o (extended)",
      "provider": "openrouter",
      "description": "GPT-4o (\"o\" for \"omni\") is OpenAI's latest AI model, supporting both text and image inputs with text outputs. It maintains the intelligence level of [GPT-4 Turbo](/models/openai/gpt-4-turbo) while being twice as fast and 50% more cost-effective. GPT-4o also offers improved performance in processing non-English languages and enhanced visual capabilities.\n\nFor benchmarking against other models, it was briefly called [\"im-also-a-good-gpt2-chatbot\"](https://twitter.com/LiamFedus/status/1790064963966370209)\n\n#multimodal",
      "weight_class": "ultraweight",
      "tier": "flagship",
      "context_length": 128000,
      "max_completion_tokens": 64000,
      "pricing": {
        "prompt_cost_per_1k": 0.006,
        "completion_cost_per_1k": 0.018000000000000002,
        "is_free": false,
        "currency": "USD"
      },
      "value_score": 1.875,
      "is_preview": false,
      "is_text_only": true,
      "estimated_params": null,
      "source_info": {
        "openrouter_raw": {
          "id": "openai/gpt-4o:extended",
          "name": "OpenAI: GPT-4o (extended)",
          "created": 1715558400,
          "description": "GPT-4o (\"o\" for \"omni\") is OpenAI's latest AI model, supporting both text and image inputs with text outputs. It maintains the intelligence level of [GPT-4 Turbo](/models/openai/gpt-4-turbo) while being twice as fast and 50% more cost-effective. GPT-4o also offers improved performance in processing non-English languages and enhanced visual capabilities.\n\nFor benchmarking against other models, it was briefly called [\"im-also-a-good-gpt2-chatbot\"](https://twitter.com/LiamFedus/status/1790064963966370209)\n\n#multimodal",
          "architecture": {
            "input_modalities": [
              "text",
              "image",
              "file"
            ],
            "output_modalities": [
              "text"
            ],
            "tokenizer": "GPT",
            "instruct_type": null
          },
          "top_provider": {
            "is_moderated": true,
            "context_length": 128000,
            "max_completion_tokens": 64000
          },
          "pricing": {
            "prompt": "0.000006",
            "completion": "0.000018",
            "image": "0.007225",
            "request": "0",
            "web_search": "0",
            "internal_reasoning": "0",
            "input_cache_read": "0",
            "input_cache_write": "0"
          },
          "canonical_slug": "openai/gpt-4o",
          "context_length": 128000,
          "hugging_face_id": null,
          "per_request_limits": null,
          "supported_parameters": [
            "frequency_penalty",
            "logit_bias",
            "logprobs",
            "max_tokens",
            "presence_penalty",
            "response_format",
            "seed",
            "stop",
            "structured_outputs",
            "temperature",
            "tool_choice",
            "tools",
            "top_logprobs",
            "top_p",
            "web_search_options"
          ]
        }
      }
    },
    {
      "id": "openai/gpt-4-turbo",
      "name": "OpenAI: GPT-4 Turbo",
      "provider": "openrouter",
      "description": "The latest GPT-4 Turbo model with vision capabilities. Vision requests can now use JSON mode and function calling.\n\nTraining data: up to December 2023.",
      "weight_class": "ultraweight",
      "tier": "flagship",
      "context_length": 128000,
      "max_completion_tokens": 4096,
      "pricing": {
        "prompt_cost_per_1k": 0.01,
        "completion_cost_per_1k": 0.030000000000000002,
        "is_free": false,
        "currency": "USD"
      },
      "value_score": 1.125,
      "is_preview": false,
      "is_text_only": true,
      "estimated_params": null,
      "source_info": {
        "openrouter_raw": {
          "id": "openai/gpt-4-turbo",
          "name": "OpenAI: GPT-4 Turbo",
          "created": 1712620800,
          "description": "The latest GPT-4 Turbo model with vision capabilities. Vision requests can now use JSON mode and function calling.\n\nTraining data: up to December 2023.",
          "architecture": {
            "input_modalities": [
              "text",
              "image"
            ],
            "output_modalities": [
              "text"
            ],
            "tokenizer": "GPT",
            "instruct_type": null
          },
          "top_provider": {
            "is_moderated": true,
            "context_length": 128000,
            "max_completion_tokens": 4096
          },
          "pricing": {
            "prompt": "0.00001",
            "completion": "0.00003",
            "image": "0.01445",
            "request": "0",
            "web_search": "0",
            "internal_reasoning": "0",
            "input_cache_read": "0",
            "input_cache_write": "0"
          },
          "canonical_slug": "openai/gpt-4-turbo",
          "context_length": 128000,
          "hugging_face_id": null,
          "per_request_limits": null,
          "supported_parameters": [
            "frequency_penalty",
            "logit_bias",
            "logprobs",
            "max_tokens",
            "presence_penalty",
            "response_format",
            "seed",
            "stop",
            "structured_outputs",
            "temperature",
            "tool_choice",
            "tools",
            "top_logprobs",
            "top_p"
          ]
        }
      }
    },
    {
      "id": "qwen/qwen-turbo",
      "name": "Qwen: Qwen-Turbo",
      "provider": "openrouter",
      "description": "Qwen-Turbo, based on Qwen2.5, is a 1M context model that provides fast speed and low cost, suitable for simple tasks.",
      "weight_class": "ultraweight",
      "tier": "premium",
      "context_length": 1000000,
      "max_completion_tokens": 8192,
      "pricing": {
        "prompt_cost_per_1k": 4.9999999999999996e-05,
        "completion_cost_per_1k": 0.00019999999999999998,
        "is_free": false,
        "currency": "USD"
      },
      "value_score": 24.750000000000004,
      "is_preview": false,
      "is_text_only": true,
      "estimated_params": null,
      "source_info": {
        "openrouter_raw": {
          "id": "qwen/qwen-turbo",
          "name": "Qwen: Qwen-Turbo",
          "created": 1738410974,
          "description": "Qwen-Turbo, based on Qwen2.5, is a 1M context model that provides fast speed and low cost, suitable for simple tasks.",
          "architecture": {
            "input_modalities": [
              "text"
            ],
            "output_modalities": [
              "text"
            ],
            "tokenizer": "Qwen",
            "instruct_type": null
          },
          "top_provider": {
            "is_moderated": false,
            "context_length": 1000000,
            "max_completion_tokens": 8192
          },
          "pricing": {
            "prompt": "0.00000005",
            "completion": "0.0000002",
            "image": "0",
            "request": "0",
            "web_search": "0",
            "internal_reasoning": "0",
            "input_cache_read": "0.00000002",
            "input_cache_write": "0"
          },
          "canonical_slug": "qwen/qwen-turbo-2024-11-01",
          "context_length": 1000000,
          "hugging_face_id": "",
          "per_request_limits": null,
          "supported_parameters": [
            "max_tokens",
            "presence_penalty",
            "response_format",
            "seed",
            "temperature",
            "tool_choice",
            "tools",
            "top_p"
          ]
        }
      }
    },
    {
      "id": "openai/gpt-oss-120b",
      "name": "OpenAI: gpt-oss-120b",
      "provider": "openrouter",
      "description": "gpt-oss-120b is an open-weight, 117B-parameter Mixture-of-Experts (MoE) language model from OpenAI designed for high-reasoning, agentic, and general-purpose production use cases. It activates 5.1B parameters per forward pass and is optimized to run on a single H100 GPU with native MXFP4 quantization. The model supports configurable reasoning depth, full chain-of-thought access, and native tool use, including function calling, browsing, and structured output generation.",
      "weight_class": "ultraweight",
      "tier": "premium",
      "context_length": 131000,
      "max_completion_tokens": 131000,
      "pricing": {
        "prompt_cost_per_1k": 7.2e-05,
        "completion_cost_per_1k": 0.00028000000000000003,
        "is_free": false,
        "currency": "USD"
      },
      "value_score": 24.750000000000004,
      "is_preview": false,
      "is_text_only": true,
      "estimated_params": "120B",
      "source_info": {
        "openrouter_raw": {
          "id": "openai/gpt-oss-120b",
          "name": "OpenAI: gpt-oss-120b",
          "created": 1754414231,
          "description": "gpt-oss-120b is an open-weight, 117B-parameter Mixture-of-Experts (MoE) language model from OpenAI designed for high-reasoning, agentic, and general-purpose production use cases. It activates 5.1B parameters per forward pass and is optimized to run on a single H100 GPU with native MXFP4 quantization. The model supports configurable reasoning depth, full chain-of-thought access, and native tool use, including function calling, browsing, and structured output generation.",
          "architecture": {
            "input_modalities": [
              "text"
            ],
            "output_modalities": [
              "text"
            ],
            "tokenizer": "GPT",
            "instruct_type": null
          },
          "top_provider": {
            "is_moderated": false,
            "context_length": 131000,
            "max_completion_tokens": 131000
          },
          "pricing": {
            "prompt": "0.000000072",
            "completion": "0.00000028",
            "image": "0",
            "request": "0",
            "web_search": "0",
            "internal_reasoning": "0",
            "input_cache_read": "0",
            "input_cache_write": "0"
          },
          "canonical_slug": "openai/gpt-oss-120b",
          "context_length": 131000,
          "hugging_face_id": "openai/gpt-oss-120b",
          "per_request_limits": null,
          "supported_parameters": [
            "frequency_penalty",
            "include_reasoning",
            "logit_bias",
            "logprobs",
            "max_tokens",
            "min_p",
            "presence_penalty",
            "reasoning",
            "repetition_penalty",
            "response_format",
            "seed",
            "stop",
            "structured_outputs",
            "temperature",
            "tool_choice",
            "tools",
            "top_k",
            "top_logprobs",
            "top_p"
          ]
        }
      }
    },
    {
      "id": "qwen/qwen3-235b-a22b-thinking-2507",
      "name": "Qwen: Qwen3 235B A22B Thinking 2507",
      "provider": "openrouter",
      "description": "Qwen3-235B-A22B-Thinking-2507 is a high-performance, open-weight Mixture-of-Experts (MoE) language model optimized for complex reasoning tasks. It activates 22B of its 235B parameters per forward pass and natively supports up to 262,144 tokens of context. This \"thinking-only\" variant enhances structured logical reasoning, mathematics, science, and long-form generation, showing strong benchmark performance across AIME, SuperGPQA, LiveCodeBench, and MMLU-Redux. It enforces a special reasoning mode (</think>) and is designed for high-token outputs (up to 81,920 tokens) in challenging domains.\n\nThe model is instruction-tuned and excels at step-by-step reasoning, tool use, agentic workflows, and multilingual tasks. This release represents the most capable open-source variant in the Qwen3-235B series, surpassing many closed models in structured reasoning use cases.",
      "weight_class": "ultraweight",
      "tier": "premium",
      "context_length": 262144,
      "max_completion_tokens": 4096,
      "pricing": {
        "prompt_cost_per_1k": 7.7968332e-05,
        "completion_cost_per_1k": 0.00031202496,
        "is_free": false,
        "currency": "USD"
      },
      "value_score": 24.750000000000004,
      "is_preview": false,
      "is_text_only": true,
      "estimated_params": "235B",
      "source_info": {
        "openrouter_raw": {
          "id": "qwen/qwen3-235b-a22b-thinking-2507",
          "name": "Qwen: Qwen3 235B A22B Thinking 2507",
          "created": 1753449557,
          "description": "Qwen3-235B-A22B-Thinking-2507 is a high-performance, open-weight Mixture-of-Experts (MoE) language model optimized for complex reasoning tasks. It activates 22B of its 235B parameters per forward pass and natively supports up to 262,144 tokens of context. This \"thinking-only\" variant enhances structured logical reasoning, mathematics, science, and long-form generation, showing strong benchmark performance across AIME, SuperGPQA, LiveCodeBench, and MMLU-Redux. It enforces a special reasoning mode (</think>) and is designed for high-token outputs (up to 81,920 tokens) in challenging domains.\n\nThe model is instruction-tuned and excels at step-by-step reasoning, tool use, agentic workflows, and multilingual tasks. This release represents the most capable open-source variant in the Qwen3-235B series, surpassing many closed models in structured reasoning use cases.",
          "architecture": {
            "input_modalities": [
              "text"
            ],
            "output_modalities": [
              "text"
            ],
            "tokenizer": "Qwen3",
            "instruct_type": "qwen3"
          },
          "top_provider": {
            "is_moderated": false,
            "context_length": 262144,
            "max_completion_tokens": null
          },
          "pricing": {
            "prompt": "0.000000077968332",
            "completion": "0.00000031202496",
            "image": "0",
            "request": "0",
            "web_search": "0",
            "internal_reasoning": "0",
            "input_cache_read": "0",
            "input_cache_write": "0"
          },
          "canonical_slug": "qwen/qwen3-235b-a22b-thinking-2507",
          "context_length": 262144,
          "hugging_face_id": "Qwen/Qwen3-235B-A22B-Thinking-2507",
          "per_request_limits": null,
          "supported_parameters": [
            "frequency_penalty",
            "include_reasoning",
            "logit_bias",
            "logprobs",
            "max_tokens",
            "min_p",
            "presence_penalty",
            "reasoning",
            "repetition_penalty",
            "response_format",
            "seed",
            "stop",
            "temperature",
            "tool_choice",
            "tools",
            "top_k",
            "top_logprobs",
            "top_p"
          ]
        }
      }
    },
    {
      "id": "qwen/qwen3-235b-a22b-2507",
      "name": "Qwen: Qwen3 235B A22B Instruct 2507",
      "provider": "openrouter",
      "description": "Qwen3-235B-A22B-Instruct-2507 is a multilingual, instruction-tuned mixture-of-experts language model based on the Qwen3-235B architecture, with 22B active parameters per forward pass. It is optimized for general-purpose text generation, including instruction following, logical reasoning, math, code, and tool usage. The model supports a native 262K context length and does not implement \"thinking mode\" (<think> blocks).\n\nCompared to its base variant, this version delivers significant gains in knowledge coverage, long-context reasoning, coding benchmarks, and alignment with open-ended tasks. It is particularly strong on multilingual understanding, math reasoning (e.g., AIME, HMMT), and alignment evaluations like Arena-Hard and WritingBench.",
      "weight_class": "ultraweight",
      "tier": "premium",
      "context_length": 262144,
      "max_completion_tokens": 4096,
      "pricing": {
        "prompt_cost_per_1k": 7.7968332e-05,
        "completion_cost_per_1k": 0.00031202496,
        "is_free": false,
        "currency": "USD"
      },
      "value_score": 24.750000000000004,
      "is_preview": false,
      "is_text_only": true,
      "estimated_params": "235B",
      "source_info": {
        "openrouter_raw": {
          "id": "qwen/qwen3-235b-a22b-2507",
          "name": "Qwen: Qwen3 235B A22B Instruct 2507",
          "created": 1753119555,
          "description": "Qwen3-235B-A22B-Instruct-2507 is a multilingual, instruction-tuned mixture-of-experts language model based on the Qwen3-235B architecture, with 22B active parameters per forward pass. It is optimized for general-purpose text generation, including instruction following, logical reasoning, math, code, and tool usage. The model supports a native 262K context length and does not implement \"thinking mode\" (<think> blocks).\n\nCompared to its base variant, this version delivers significant gains in knowledge coverage, long-context reasoning, coding benchmarks, and alignment with open-ended tasks. It is particularly strong on multilingual understanding, math reasoning (e.g., AIME, HMMT), and alignment evaluations like Arena-Hard and WritingBench.",
          "architecture": {
            "input_modalities": [
              "text"
            ],
            "output_modalities": [
              "text"
            ],
            "tokenizer": "Qwen3",
            "instruct_type": null
          },
          "top_provider": {
            "is_moderated": false,
            "context_length": 262144,
            "max_completion_tokens": null
          },
          "pricing": {
            "prompt": "0.000000077968332",
            "completion": "0.00000031202496",
            "image": "0",
            "request": "0",
            "web_search": "0",
            "internal_reasoning": "0",
            "input_cache_read": "0",
            "input_cache_write": "0"
          },
          "canonical_slug": "qwen/qwen3-235b-a22b-07-25",
          "context_length": 262144,
          "hugging_face_id": "Qwen/Qwen3-235B-A22B-Instruct-2507",
          "per_request_limits": null,
          "supported_parameters": [
            "frequency_penalty",
            "logit_bias",
            "logprobs",
            "max_tokens",
            "min_p",
            "presence_penalty",
            "repetition_penalty",
            "response_format",
            "seed",
            "stop",
            "structured_outputs",
            "temperature",
            "tool_choice",
            "tools",
            "top_k",
            "top_logprobs",
            "top_p"
          ]
        }
      }
    },
    {
      "id": "cohere/command-r-08-2024",
      "name": "Cohere: Command R (08-2024)",
      "provider": "openrouter",
      "description": "command-r-08-2024 is an update of the [Command R](/models/cohere/command-r) with improved performance for multilingual retrieval-augmented generation (RAG) and tool use. More broadly, it is better at math, code and reasoning and is competitive with the previous version of the larger Command R+ model.\n\nRead the launch post [here](https://docs.cohere.com/changelog/command-gets-refreshed).\n\nUse of this model is subject to Cohere's [Usage Policy](https://docs.cohere.com/docs/usage-policy) and [SaaS Agreement](https://cohere.com/saas-agreement).",
      "weight_class": "ultraweight",
      "tier": "premium",
      "context_length": 128000,
      "max_completion_tokens": 4000,
      "pricing": {
        "prompt_cost_per_1k": 0.00015,
        "completion_cost_per_1k": 0.0006,
        "is_free": false,
        "currency": "USD"
      },
      "value_score": 24.750000000000004,
      "is_preview": false,
      "is_text_only": true,
      "estimated_params": null,
      "source_info": {
        "openrouter_raw": {
          "id": "cohere/command-r-08-2024",
          "name": "Cohere: Command R (08-2024)",
          "created": 1724976000,
          "description": "command-r-08-2024 is an update of the [Command R](/models/cohere/command-r) with improved performance for multilingual retrieval-augmented generation (RAG) and tool use. More broadly, it is better at math, code and reasoning and is competitive with the previous version of the larger Command R+ model.\n\nRead the launch post [here](https://docs.cohere.com/changelog/command-gets-refreshed).\n\nUse of this model is subject to Cohere's [Usage Policy](https://docs.cohere.com/docs/usage-policy) and [SaaS Agreement](https://cohere.com/saas-agreement).",
          "architecture": {
            "input_modalities": [
              "text"
            ],
            "output_modalities": [
              "text"
            ],
            "tokenizer": "Cohere",
            "instruct_type": null
          },
          "top_provider": {
            "is_moderated": true,
            "context_length": 128000,
            "max_completion_tokens": 4000
          },
          "pricing": {
            "prompt": "0.00000015",
            "completion": "0.0000006",
            "image": "0",
            "request": "0",
            "web_search": "0",
            "internal_reasoning": "0",
            "input_cache_read": "0",
            "input_cache_write": "0"
          },
          "canonical_slug": "cohere/command-r-08-2024",
          "context_length": 128000,
          "hugging_face_id": null,
          "per_request_limits": null,
          "supported_parameters": [
            "frequency_penalty",
            "max_tokens",
            "presence_penalty",
            "response_format",
            "seed",
            "stop",
            "structured_outputs",
            "temperature",
            "tools",
            "top_k",
            "top_p"
          ]
        }
      }
    },
    {
      "id": "deepseek/deepseek-r1-0528",
      "name": "DeepSeek: R1 0528",
      "provider": "openrouter",
      "description": "May 28th update to the [original DeepSeek R1](/deepseek/deepseek-r1) Performance on par with [OpenAI o1](/openai/o1), but open-sourced and with fully open reasoning tokens. It's 671B parameters in size, with 37B active in an inference pass.\n\nFully open-source model.",
      "weight_class": "ultraweight",
      "tier": "premium",
      "context_length": 163840,
      "max_completion_tokens": 4096,
      "pricing": {
        "prompt_cost_per_1k": 0.0001999188,
        "completion_cost_per_1k": 0.0008000640000000001,
        "is_free": false,
        "currency": "USD"
      },
      "value_score": 24.750000000000004,
      "is_preview": false,
      "is_text_only": true,
      "estimated_params": "671B",
      "source_info": {
        "openrouter_raw": {
          "id": "deepseek/deepseek-r1-0528",
          "name": "DeepSeek: R1 0528",
          "created": 1748455170,
          "description": "May 28th update to the [original DeepSeek R1](/deepseek/deepseek-r1) Performance on par with [OpenAI o1](/openai/o1), but open-sourced and with fully open reasoning tokens. It's 671B parameters in size, with 37B active in an inference pass.\n\nFully open-source model.",
          "architecture": {
            "input_modalities": [
              "text"
            ],
            "output_modalities": [
              "text"
            ],
            "tokenizer": "DeepSeek",
            "instruct_type": "deepseek-r1"
          },
          "top_provider": {
            "is_moderated": false,
            "context_length": 163840,
            "max_completion_tokens": null
          },
          "pricing": {
            "prompt": "0.0000001999188",
            "completion": "0.000000800064",
            "image": "0",
            "request": "0",
            "web_search": "0",
            "internal_reasoning": "0",
            "input_cache_read": "0",
            "input_cache_write": "0"
          },
          "canonical_slug": "deepseek/deepseek-r1-0528",
          "context_length": 163840,
          "hugging_face_id": "deepseek-ai/DeepSeek-R1-0528",
          "per_request_limits": null,
          "supported_parameters": [
            "frequency_penalty",
            "include_reasoning",
            "logit_bias",
            "logprobs",
            "max_tokens",
            "min_p",
            "presence_penalty",
            "reasoning",
            "repetition_penalty",
            "response_format",
            "seed",
            "stop",
            "structured_outputs",
            "temperature",
            "tool_choice",
            "tools",
            "top_k",
            "top_logprobs",
            "top_p"
          ]
        }
      }
    },
    {
      "id": "tngtech/deepseek-r1t-chimera",
      "name": "TNG: DeepSeek R1T Chimera",
      "provider": "openrouter",
      "description": "DeepSeek-R1T-Chimera is created by merging DeepSeek-R1 and DeepSeek-V3 (0324), combining the reasoning capabilities of R1 with the token efficiency improvements of V3. It is based on a DeepSeek-MoE Transformer architecture and is optimized for general text generation tasks.\n\nThe model merges pretrained weights from both source models to balance performance across reasoning, efficiency, and instruction-following tasks. It is released under the MIT license and intended for research and commercial use.",
      "weight_class": "ultraweight",
      "tier": "premium",
      "context_length": 163840,
      "max_completion_tokens": 4096,
      "pricing": {
        "prompt_cost_per_1k": 0.0001999188,
        "completion_cost_per_1k": 0.0008000640000000001,
        "is_free": false,
        "currency": "USD"
      },
      "value_score": 24.750000000000004,
      "is_preview": false,
      "is_text_only": true,
      "estimated_params": null,
      "source_info": {
        "openrouter_raw": {
          "id": "tngtech/deepseek-r1t-chimera",
          "name": "TNG: DeepSeek R1T Chimera",
          "created": 1745760875,
          "description": "DeepSeek-R1T-Chimera is created by merging DeepSeek-R1 and DeepSeek-V3 (0324), combining the reasoning capabilities of R1 with the token efficiency improvements of V3. It is based on a DeepSeek-MoE Transformer architecture and is optimized for general text generation tasks.\n\nThe model merges pretrained weights from both source models to balance performance across reasoning, efficiency, and instruction-following tasks. It is released under the MIT license and intended for research and commercial use.",
          "architecture": {
            "input_modalities": [
              "text"
            ],
            "output_modalities": [
              "text"
            ],
            "tokenizer": "DeepSeek",
            "instruct_type": null
          },
          "top_provider": {
            "is_moderated": false,
            "context_length": 163840,
            "max_completion_tokens": null
          },
          "pricing": {
            "prompt": "0.0000001999188",
            "completion": "0.000000800064",
            "image": "0",
            "request": "0",
            "web_search": "0",
            "internal_reasoning": "0",
            "input_cache_read": "0",
            "input_cache_write": "0"
          },
          "canonical_slug": "tngtech/deepseek-r1t-chimera",
          "context_length": 163840,
          "hugging_face_id": "tngtech/DeepSeek-R1T-Chimera",
          "per_request_limits": null,
          "supported_parameters": [
            "frequency_penalty",
            "include_reasoning",
            "logit_bias",
            "logprobs",
            "max_tokens",
            "min_p",
            "presence_penalty",
            "reasoning",
            "repetition_penalty",
            "seed",
            "stop",
            "temperature",
            "top_k",
            "top_logprobs",
            "top_p"
          ]
        }
      }
    },
    {
      "id": "deepseek/deepseek-chat-v3-0324",
      "name": "DeepSeek: DeepSeek V3 0324",
      "provider": "openrouter",
      "description": "DeepSeek V3, a 685B-parameter, mixture-of-experts model, is the latest iteration of the flagship chat model family from the DeepSeek team.\n\nIt succeeds the [DeepSeek V3](/deepseek/deepseek-chat-v3) model and performs really well on a variety of tasks.",
      "weight_class": "ultraweight",
      "tier": "premium",
      "context_length": 163840,
      "max_completion_tokens": 4096,
      "pricing": {
        "prompt_cost_per_1k": 0.0001999188,
        "completion_cost_per_1k": 0.0008000640000000001,
        "is_free": false,
        "currency": "USD"
      },
      "value_score": 24.750000000000004,
      "is_preview": false,
      "is_text_only": true,
      "estimated_params": "685B",
      "source_info": {
        "openrouter_raw": {
          "id": "deepseek/deepseek-chat-v3-0324",
          "name": "DeepSeek: DeepSeek V3 0324",
          "created": 1742824755,
          "description": "DeepSeek V3, a 685B-parameter, mixture-of-experts model, is the latest iteration of the flagship chat model family from the DeepSeek team.\n\nIt succeeds the [DeepSeek V3](/deepseek/deepseek-chat-v3) model and performs really well on a variety of tasks.",
          "architecture": {
            "input_modalities": [
              "text"
            ],
            "output_modalities": [
              "text"
            ],
            "tokenizer": "DeepSeek",
            "instruct_type": null
          },
          "top_provider": {
            "is_moderated": false,
            "context_length": 163840,
            "max_completion_tokens": null
          },
          "pricing": {
            "prompt": "0.0000001999188",
            "completion": "0.000000800064",
            "image": "0",
            "request": "0",
            "web_search": "0",
            "internal_reasoning": "0",
            "input_cache_read": "0",
            "input_cache_write": "0"
          },
          "canonical_slug": "deepseek/deepseek-chat-v3-0324",
          "context_length": 163840,
          "hugging_face_id": "deepseek-ai/DeepSeek-V3-0324",
          "per_request_limits": null,
          "supported_parameters": [
            "frequency_penalty",
            "logit_bias",
            "logprobs",
            "max_tokens",
            "min_p",
            "presence_penalty",
            "repetition_penalty",
            "response_format",
            "seed",
            "stop",
            "structured_outputs",
            "temperature",
            "tool_choice",
            "tools",
            "top_k",
            "top_logprobs",
            "top_p"
          ]
        }
      }
    },
    {
      "id": "meta-llama/llama-3.3-8b-instruct:free",
      "name": "Meta: Llama 3.3 8B Instruct (free)",
      "provider": "openrouter",
      "description": "A lightweight and ultra-fast variant of Llama 3.3 70B, for use when quick response times are needed most.",
      "weight_class": "middleweight",
      "tier": "balanced",
      "context_length": 128000,
      "max_completion_tokens": 4028,
      "pricing": {
        "prompt_cost_per_1k": 0.0,
        "completion_cost_per_1k": 0.0,
        "is_free": true,
        "currency": "USD"
      },
      "value_score": 16.5,
      "is_preview": false,
      "is_text_only": true,
      "estimated_params": "8B",
      "source_info": {
        "openrouter_raw": {
          "id": "meta-llama/llama-3.3-8b-instruct:free",
          "name": "Meta: Llama 3.3 8B Instruct (free)",
          "created": 1747230154,
          "description": "A lightweight and ultra-fast variant of Llama 3.3 70B, for use when quick response times are needed most.",
          "architecture": {
            "input_modalities": [
              "text"
            ],
            "output_modalities": [
              "text"
            ],
            "tokenizer": "Llama3",
            "instruct_type": null
          },
          "top_provider": {
            "is_moderated": true,
            "context_length": 128000,
            "max_completion_tokens": 4028
          },
          "pricing": {
            "prompt": "0",
            "completion": "0",
            "image": "0",
            "request": "0",
            "web_search": "0",
            "internal_reasoning": "0",
            "input_cache_read": "0",
            "input_cache_write": "0"
          },
          "canonical_slug": "meta-llama/llama-3.3-8b-instruct",
          "context_length": 128000,
          "hugging_face_id": "",
          "per_request_limits": null,
          "supported_parameters": [
            "max_tokens",
            "repetition_penalty",
            "response_format",
            "structured_outputs",
            "temperature",
            "tool_choice",
            "tools",
            "top_k",
            "top_p"
          ]
        }
      }
    },
    {
      "id": "meta-llama/llama-3.1-8b-instruct",
      "name": "Meta: Llama 3.1 8B Instruct",
      "provider": "openrouter",
      "description": "Meta's latest class of model (Llama 3.1) launched with a variety of sizes & flavors. This 8B instruct-tuned version is fast and efficient.\n\nIt has demonstrated strong performance compared to leading closed-source models in human evaluations.\n\nTo read more about the model release, [click here](https://ai.meta.com/blog/meta-llama-3-1/). Usage of this model is subject to [Meta's Acceptable Use Policy](https://llama.meta.com/llama3/use-policy/).",
      "weight_class": "middleweight",
      "tier": "balanced",
      "context_length": 131072,
      "max_completion_tokens": 16384,
      "pricing": {
        "prompt_cost_per_1k": 1.4999999999999999e-05,
        "completion_cost_per_1k": 2e-05,
        "is_free": false,
        "currency": "USD"
      },
      "value_score": 16.5,
      "is_preview": false,
      "is_text_only": true,
      "estimated_params": "8B",
      "source_info": {
        "openrouter_raw": {
          "id": "meta-llama/llama-3.1-8b-instruct",
          "name": "Meta: Llama 3.1 8B Instruct",
          "created": 1721692800,
          "description": "Meta's latest class of model (Llama 3.1) launched with a variety of sizes & flavors. This 8B instruct-tuned version is fast and efficient.\n\nIt has demonstrated strong performance compared to leading closed-source models in human evaluations.\n\nTo read more about the model release, [click here](https://ai.meta.com/blog/meta-llama-3-1/). Usage of this model is subject to [Meta's Acceptable Use Policy](https://llama.meta.com/llama3/use-policy/).",
          "architecture": {
            "input_modalities": [
              "text"
            ],
            "output_modalities": [
              "text"
            ],
            "tokenizer": "Llama3",
            "instruct_type": "llama3"
          },
          "top_provider": {
            "is_moderated": false,
            "context_length": 131072,
            "max_completion_tokens": 16384
          },
          "pricing": {
            "prompt": "0.000000015",
            "completion": "0.00000002",
            "image": "0",
            "request": "0",
            "web_search": "0",
            "internal_reasoning": "0",
            "input_cache_read": "0",
            "input_cache_write": "0"
          },
          "canonical_slug": "meta-llama/llama-3.1-8b-instruct",
          "context_length": 131072,
          "hugging_face_id": "meta-llama/Meta-Llama-3.1-8B-Instruct",
          "per_request_limits": null,
          "supported_parameters": [
            "frequency_penalty",
            "logit_bias",
            "logprobs",
            "max_tokens",
            "min_p",
            "presence_penalty",
            "repetition_penalty",
            "response_format",
            "seed",
            "stop",
            "structured_outputs",
            "temperature",
            "tool_choice",
            "tools",
            "top_k",
            "top_logprobs",
            "top_p"
          ]
        }
      }
    },
    {
      "id": "mistralai/mistral-nemo",
      "name": "Mistral: Mistral Nemo",
      "provider": "openrouter",
      "description": "A 12B parameter model with a 128k token context length built by Mistral in collaboration with NVIDIA.\n\nThe model is multilingual, supporting English, French, German, Spanish, Italian, Portuguese, Chinese, Japanese, Korean, Arabic, and Hindi.\n\nIt supports function calling and is released under the Apache 2.0 license.",
      "weight_class": "middleweight",
      "tier": "balanced",
      "context_length": 131072,
      "max_completion_tokens": 128000,
      "pricing": {
        "prompt_cost_per_1k": 1e-05,
        "completion_cost_per_1k": 4.0003200000000004e-05,
        "is_free": false,
        "currency": "USD"
      },
      "value_score": 16.5,
      "is_preview": false,
      "is_text_only": true,
      "estimated_params": "12B",
      "source_info": {
        "openrouter_raw": {
          "id": "mistralai/mistral-nemo",
          "name": "Mistral: Mistral Nemo",
          "created": 1721347200,
          "description": "A 12B parameter model with a 128k token context length built by Mistral in collaboration with NVIDIA.\n\nThe model is multilingual, supporting English, French, German, Spanish, Italian, Portuguese, Chinese, Japanese, Korean, Arabic, and Hindi.\n\nIt supports function calling and is released under the Apache 2.0 license.",
          "architecture": {
            "input_modalities": [
              "text"
            ],
            "output_modalities": [
              "text"
            ],
            "tokenizer": "Mistral",
            "instruct_type": "mistral"
          },
          "top_provider": {
            "is_moderated": false,
            "context_length": 131072,
            "max_completion_tokens": 128000
          },
          "pricing": {
            "prompt": "0.00000001",
            "completion": "0.0000000400032",
            "image": "0",
            "request": "0",
            "web_search": "0",
            "internal_reasoning": "0",
            "input_cache_read": "0",
            "input_cache_write": "0"
          },
          "canonical_slug": "mistralai/mistral-nemo",
          "context_length": 131072,
          "hugging_face_id": "mistralai/Mistral-Nemo-Instruct-2407",
          "per_request_limits": null,
          "supported_parameters": [
            "frequency_penalty",
            "logit_bias",
            "logprobs",
            "max_tokens",
            "min_p",
            "presence_penalty",
            "repetition_penalty",
            "response_format",
            "seed",
            "stop",
            "structured_outputs",
            "temperature",
            "tool_choice",
            "tools",
            "top_k",
            "top_logprobs",
            "top_p"
          ]
        }
      }
    },
    {
      "id": "meta-llama/llama-guard-3-8b",
      "name": "Llama Guard 3 8B",
      "provider": "openrouter",
      "description": "Llama Guard 3 is a Llama-3.1-8B pretrained model, fine-tuned for content safety classification. Similar to previous versions, it can be used to classify content in both LLM inputs (prompt classification) and in LLM responses (response classification). It acts as an LLM \u2013 it generates text in its output that indicates whether a given prompt or response is safe or unsafe, and if unsafe, it also lists the content categories violated.\n\nLlama Guard 3 was aligned to safeguard against the MLCommons standardized hazards taxonomy and designed to support Llama 3.1 capabilities. Specifically, it provides content moderation in 8 languages, and was optimized to support safety and security for search and code interpreter tool calls.\n",
      "weight_class": "middleweight",
      "tier": "balanced",
      "context_length": 131072,
      "max_completion_tokens": 4096,
      "pricing": {
        "prompt_cost_per_1k": 2e-05,
        "completion_cost_per_1k": 5.9999999999999995e-05,
        "is_free": false,
        "currency": "USD"
      },
      "value_score": 16.5,
      "is_preview": false,
      "is_text_only": true,
      "estimated_params": "8B",
      "source_info": {
        "openrouter_raw": {
          "id": "meta-llama/llama-guard-3-8b",
          "name": "Llama Guard 3 8B",
          "created": 1739401318,
          "description": "Llama Guard 3 is a Llama-3.1-8B pretrained model, fine-tuned for content safety classification. Similar to previous versions, it can be used to classify content in both LLM inputs (prompt classification) and in LLM responses (response classification). It acts as an LLM \u2013 it generates text in its output that indicates whether a given prompt or response is safe or unsafe, and if unsafe, it also lists the content categories violated.\n\nLlama Guard 3 was aligned to safeguard against the MLCommons standardized hazards taxonomy and designed to support Llama 3.1 capabilities. Specifically, it provides content moderation in 8 languages, and was optimized to support safety and security for search and code interpreter tool calls.\n",
          "architecture": {
            "input_modalities": [
              "text"
            ],
            "output_modalities": [
              "text"
            ],
            "tokenizer": "Llama3",
            "instruct_type": "none"
          },
          "top_provider": {
            "is_moderated": false,
            "context_length": 131072,
            "max_completion_tokens": null
          },
          "pricing": {
            "prompt": "0.00000002",
            "completion": "0.00000006",
            "image": "0",
            "request": "0",
            "web_search": "0",
            "internal_reasoning": "0",
            "input_cache_read": "0",
            "input_cache_write": "0"
          },
          "canonical_slug": "meta-llama/llama-guard-3-8b",
          "context_length": 131072,
          "hugging_face_id": "meta-llama/Llama-Guard-3-8B",
          "per_request_limits": null,
          "supported_parameters": [
            "frequency_penalty",
            "logit_bias",
            "logprobs",
            "max_tokens",
            "min_p",
            "presence_penalty",
            "repetition_penalty",
            "response_format",
            "seed",
            "stop",
            "temperature",
            "top_k",
            "top_logprobs",
            "top_p"
          ]
        }
      }
    },
    {
      "id": "deepseek/deepseek-r1-0528-qwen3-8b",
      "name": "DeepSeek: Deepseek R1 0528 Qwen3 8B",
      "provider": "openrouter",
      "description": "DeepSeek-R1-0528 is a lightly upgraded release of DeepSeek R1 that taps more compute and smarter post-training tricks, pushing its reasoning and inference to the brink of flagship models like O3 and Gemini 2.5 Pro.\nIt now tops math, programming, and logic leaderboards, showcasing a step-change in depth-of-thought.\nThe distilled variant, DeepSeek-R1-0528-Qwen3-8B, transfers this chain-of-thought into an 8 B-parameter form, beating standard Qwen3 8B by +10 pp and tying the 235 B \u201cthinking\u201d giant on AIME 2024.",
      "weight_class": "middleweight",
      "tier": "balanced",
      "context_length": 131072,
      "max_completion_tokens": 4096,
      "pricing": {
        "prompt_cost_per_1k": 1.703012e-05,
        "completion_cost_per_1k": 6.81536e-05,
        "is_free": false,
        "currency": "USD"
      },
      "value_score": 16.5,
      "is_preview": false,
      "is_text_only": true,
      "estimated_params": "8B",
      "source_info": {
        "openrouter_raw": {
          "id": "deepseek/deepseek-r1-0528-qwen3-8b",
          "name": "DeepSeek: Deepseek R1 0528 Qwen3 8B",
          "created": 1748538543,
          "description": "DeepSeek-R1-0528 is a lightly upgraded release of DeepSeek R1 that taps more compute and smarter post-training tricks, pushing its reasoning and inference to the brink of flagship models like O3 and Gemini 2.5 Pro.\nIt now tops math, programming, and logic leaderboards, showcasing a step-change in depth-of-thought.\nThe distilled variant, DeepSeek-R1-0528-Qwen3-8B, transfers this chain-of-thought into an 8 B-parameter form, beating standard Qwen3 8B by +10 pp and tying the 235 B \u201cthinking\u201d giant on AIME 2024.",
          "architecture": {
            "input_modalities": [
              "text"
            ],
            "output_modalities": [
              "text"
            ],
            "tokenizer": "Qwen",
            "instruct_type": "deepseek-r1"
          },
          "top_provider": {
            "is_moderated": false,
            "context_length": 131072,
            "max_completion_tokens": null
          },
          "pricing": {
            "prompt": "0.00000001703012",
            "completion": "0.0000000681536",
            "image": "0",
            "request": "0",
            "web_search": "0",
            "internal_reasoning": "0",
            "input_cache_read": "0",
            "input_cache_write": "0"
          },
          "canonical_slug": "deepseek/deepseek-r1-0528-qwen3-8b",
          "context_length": 131072,
          "hugging_face_id": "deepseek-ai/deepseek-r1-0528-qwen3-8b",
          "per_request_limits": null,
          "supported_parameters": [
            "frequency_penalty",
            "include_reasoning",
            "logit_bias",
            "logprobs",
            "max_tokens",
            "min_p",
            "presence_penalty",
            "reasoning",
            "repetition_penalty",
            "seed",
            "stop",
            "temperature",
            "top_k",
            "top_logprobs",
            "top_p"
          ]
        }
      }
    },
    {
      "id": "mistralai/devstral-small-2505",
      "name": "Mistral: Devstral Small 2505",
      "provider": "openrouter",
      "description": "Devstral-Small-2505 is a 24B parameter agentic LLM fine-tuned from Mistral-Small-3.1, jointly developed by Mistral AI and All Hands AI for advanced software engineering tasks. It is optimized for codebase exploration, multi-file editing, and integration into coding agents, achieving state-of-the-art results on SWE-Bench Verified (46.8%).\n\nDevstral supports a 128k context window and uses a custom Tekken tokenizer. It is text-only, with the vision encoder removed, and is suitable for local deployment on high-end consumer hardware (e.g., RTX 4090, 32GB RAM Macs). Devstral is best used in agentic workflows via the OpenHands scaffold and is compatible with inference frameworks like vLLM, Transformers, and Ollama. It is released under the Apache 2.0 license.",
      "weight_class": "middleweight",
      "tier": "balanced",
      "context_length": 131072,
      "max_completion_tokens": 4096,
      "pricing": {
        "prompt_cost_per_1k": 1.999188e-05,
        "completion_cost_per_1k": 8.000640000000001e-05,
        "is_free": false,
        "currency": "USD"
      },
      "value_score": 16.5,
      "is_preview": false,
      "is_text_only": true,
      "estimated_params": "24B",
      "source_info": {
        "openrouter_raw": {
          "id": "mistralai/devstral-small-2505",
          "name": "Mistral: Devstral Small 2505",
          "created": 1747837379,
          "description": "Devstral-Small-2505 is a 24B parameter agentic LLM fine-tuned from Mistral-Small-3.1, jointly developed by Mistral AI and All Hands AI for advanced software engineering tasks. It is optimized for codebase exploration, multi-file editing, and integration into coding agents, achieving state-of-the-art results on SWE-Bench Verified (46.8%).\n\nDevstral supports a 128k context window and uses a custom Tekken tokenizer. It is text-only, with the vision encoder removed, and is suitable for local deployment on high-end consumer hardware (e.g., RTX 4090, 32GB RAM Macs). Devstral is best used in agentic workflows via the OpenHands scaffold and is compatible with inference frameworks like vLLM, Transformers, and Ollama. It is released under the Apache 2.0 license.",
          "architecture": {
            "input_modalities": [
              "text"
            ],
            "output_modalities": [
              "text"
            ],
            "tokenizer": "Mistral",
            "instruct_type": null
          },
          "top_provider": {
            "is_moderated": false,
            "context_length": 131072,
            "max_completion_tokens": null
          },
          "pricing": {
            "prompt": "0.00000001999188",
            "completion": "0.0000000800064",
            "image": "0",
            "request": "0",
            "web_search": "0",
            "internal_reasoning": "0",
            "input_cache_read": "0",
            "input_cache_write": "0"
          },
          "canonical_slug": "mistralai/devstral-small-2505",
          "context_length": 131072,
          "hugging_face_id": "mistralai/Devstral-Small-2505",
          "per_request_limits": null,
          "supported_parameters": [
            "frequency_penalty",
            "logit_bias",
            "logprobs",
            "max_tokens",
            "min_p",
            "presence_penalty",
            "repetition_penalty",
            "response_format",
            "seed",
            "stop",
            "structured_outputs",
            "temperature",
            "tool_choice",
            "tools",
            "top_k",
            "top_logprobs",
            "top_p"
          ]
        }
      }
    },
    {
      "id": "qwen/qwen3-8b",
      "name": "Qwen: Qwen3 8B",
      "provider": "openrouter",
      "description": "Qwen3-8B is a dense 8.2B parameter causal language model from the Qwen3 series, designed for both reasoning-heavy tasks and efficient dialogue. It supports seamless switching between \"thinking\" mode for math, coding, and logical inference, and \"non-thinking\" mode for general conversation. The model is fine-tuned for instruction-following, agent integration, creative writing, and multilingual use across 100+ languages and dialects. It natively supports a 32K token context window and can extend to 131K tokens with YaRN scaling.",
      "weight_class": "middleweight",
      "tier": "balanced",
      "context_length": 128000,
      "max_completion_tokens": 20000,
      "pricing": {
        "prompt_cost_per_1k": 3.5000000000000004e-05,
        "completion_cost_per_1k": 0.000138,
        "is_free": false,
        "currency": "USD"
      },
      "value_score": 16.5,
      "is_preview": false,
      "is_text_only": true,
      "estimated_params": "8B",
      "source_info": {
        "openrouter_raw": {
          "id": "qwen/qwen3-8b",
          "name": "Qwen: Qwen3 8B",
          "created": 1745876632,
          "description": "Qwen3-8B is a dense 8.2B parameter causal language model from the Qwen3 series, designed for both reasoning-heavy tasks and efficient dialogue. It supports seamless switching between \"thinking\" mode for math, coding, and logical inference, and \"non-thinking\" mode for general conversation. The model is fine-tuned for instruction-following, agent integration, creative writing, and multilingual use across 100+ languages and dialects. It natively supports a 32K token context window and can extend to 131K tokens with YaRN scaling.",
          "architecture": {
            "input_modalities": [
              "text"
            ],
            "output_modalities": [
              "text"
            ],
            "tokenizer": "Qwen3",
            "instruct_type": "qwen3"
          },
          "top_provider": {
            "is_moderated": false,
            "context_length": 128000,
            "max_completion_tokens": 20000
          },
          "pricing": {
            "prompt": "0.000000035",
            "completion": "0.000000138",
            "image": "0",
            "request": "0",
            "web_search": "0",
            "internal_reasoning": "0",
            "input_cache_read": "0",
            "input_cache_write": "0"
          },
          "canonical_slug": "qwen/qwen3-8b-04-28",
          "context_length": 128000,
          "hugging_face_id": "Qwen/Qwen3-8B",
          "per_request_limits": null,
          "supported_parameters": [
            "frequency_penalty",
            "include_reasoning",
            "logit_bias",
            "max_tokens",
            "min_p",
            "presence_penalty",
            "reasoning",
            "repetition_penalty",
            "seed",
            "stop",
            "temperature",
            "top_k",
            "top_p"
          ]
        }
      }
    },
    {
      "id": "cohere/command-r7b-12-2024",
      "name": "Cohere: Command R7B (12-2024)",
      "provider": "openrouter",
      "description": "Command R7B (12-2024) is a small, fast update of the Command R+ model, delivered in December 2024. It excels at RAG, tool use, agents, and similar tasks requiring complex reasoning and multiple steps.\n\nUse of this model is subject to Cohere's [Usage Policy](https://docs.cohere.com/docs/usage-policy) and [SaaS Agreement](https://cohere.com/saas-agreement).",
      "weight_class": "middleweight",
      "tier": "balanced",
      "context_length": 128000,
      "max_completion_tokens": 4000,
      "pricing": {
        "prompt_cost_per_1k": 3.75e-05,
        "completion_cost_per_1k": 0.00015,
        "is_free": false,
        "currency": "USD"
      },
      "value_score": 16.5,
      "is_preview": false,
      "is_text_only": true,
      "estimated_params": "7B",
      "source_info": {
        "openrouter_raw": {
          "id": "cohere/command-r7b-12-2024",
          "name": "Cohere: Command R7B (12-2024)",
          "created": 1734158152,
          "description": "Command R7B (12-2024) is a small, fast update of the Command R+ model, delivered in December 2024. It excels at RAG, tool use, agents, and similar tasks requiring complex reasoning and multiple steps.\n\nUse of this model is subject to Cohere's [Usage Policy](https://docs.cohere.com/docs/usage-policy) and [SaaS Agreement](https://cohere.com/saas-agreement).",
          "architecture": {
            "input_modalities": [
              "text"
            ],
            "output_modalities": [
              "text"
            ],
            "tokenizer": "Cohere",
            "instruct_type": null
          },
          "top_provider": {
            "is_moderated": true,
            "context_length": 128000,
            "max_completion_tokens": 4000
          },
          "pricing": {
            "prompt": "0.0000000375",
            "completion": "0.00000015",
            "image": "0",
            "request": "0",
            "web_search": "0",
            "internal_reasoning": "0",
            "input_cache_read": "0",
            "input_cache_write": "0"
          },
          "canonical_slug": "cohere/command-r7b-12-2024",
          "context_length": 128000,
          "hugging_face_id": "",
          "per_request_limits": null,
          "supported_parameters": [
            "frequency_penalty",
            "max_tokens",
            "presence_penalty",
            "response_format",
            "seed",
            "stop",
            "structured_outputs",
            "temperature",
            "top_k",
            "top_p"
          ]
        }
      }
    },
    {
      "id": "z-ai/glm-4.5-air:free",
      "name": "Z.AI: GLM 4.5 Air (free)",
      "provider": "openrouter",
      "description": "GLM-4.5-Air is the lightweight variant of our latest flagship model family, also purpose-built for agent-centric applications. Like GLM-4.5, it adopts the Mixture-of-Experts (MoE) architecture but with a more compact parameter size. GLM-4.5-Air also supports hybrid inference modes, offering a \"thinking mode\" for advanced reasoning and tool use, and a \"non-thinking mode\" for real-time interaction. Users can control the reasoning behaviour with the `reasoning` `enabled` boolean. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)",
      "weight_class": "ultraweight",
      "tier": "budget",
      "context_length": 131072,
      "max_completion_tokens": 4096,
      "pricing": {
        "prompt_cost_per_1k": 0.0,
        "completion_cost_per_1k": 0.0,
        "is_free": true,
        "currency": "USD"
      },
      "value_score": 24.750000000000004,
      "is_preview": true,
      "is_text_only": true,
      "estimated_params": null,
      "source_info": {
        "openrouter_raw": {
          "id": "z-ai/glm-4.5-air:free",
          "name": "Z.AI: GLM 4.5 Air (free)",
          "created": 1753471258,
          "description": "GLM-4.5-Air is the lightweight variant of our latest flagship model family, also purpose-built for agent-centric applications. Like GLM-4.5, it adopts the Mixture-of-Experts (MoE) architecture but with a more compact parameter size. GLM-4.5-Air also supports hybrid inference modes, offering a \"thinking mode\" for advanced reasoning and tool use, and a \"non-thinking mode\" for real-time interaction. Users can control the reasoning behaviour with the `reasoning` `enabled` boolean. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)",
          "architecture": {
            "input_modalities": [
              "text"
            ],
            "output_modalities": [
              "text"
            ],
            "tokenizer": "Other",
            "instruct_type": null
          },
          "top_provider": {
            "is_moderated": false,
            "context_length": 131072,
            "max_completion_tokens": null
          },
          "pricing": {
            "prompt": "0",
            "completion": "0",
            "image": "0",
            "request": "0",
            "web_search": "0",
            "internal_reasoning": "0",
            "input_cache_read": "0",
            "input_cache_write": "0"
          },
          "canonical_slug": "z-ai/glm-4.5-air",
          "context_length": 131072,
          "hugging_face_id": "zai-org/GLM-4.5-Air",
          "per_request_limits": null,
          "supported_parameters": [
            "frequency_penalty",
            "include_reasoning",
            "logit_bias",
            "logprobs",
            "max_tokens",
            "min_p",
            "presence_penalty",
            "reasoning",
            "repetition_penalty",
            "seed",
            "stop",
            "temperature",
            "tool_choice",
            "tools",
            "top_k",
            "top_logprobs",
            "top_p"
          ]
        }
      }
    },
    {
      "id": "qwen/qwen3-coder:free",
      "name": "Qwen: Qwen3 Coder 480B A35B (free)",
      "provider": "openrouter",
      "description": "Qwen3-Coder-480B-A35B-Instruct is a Mixture-of-Experts (MoE) code generation model developed by the Qwen team. It is optimized for agentic coding tasks such as function calling, tool use, and long-context reasoning over repositories. The model features 480 billion total parameters, with 35 billion active per forward pass (8 out of 160 experts).\n\nPricing for the Alibaba endpoints varies by context length. Once a request is greater than 128k input tokens, the higher pricing is used.",
      "weight_class": "ultraweight",
      "tier": "budget",
      "context_length": 262144,
      "max_completion_tokens": 4096,
      "pricing": {
        "prompt_cost_per_1k": 0.0,
        "completion_cost_per_1k": 0.0,
        "is_free": true,
        "currency": "USD"
      },
      "value_score": 24.750000000000004,
      "is_preview": true,
      "is_text_only": true,
      "estimated_params": "480B",
      "source_info": {
        "openrouter_raw": {
          "id": "qwen/qwen3-coder:free",
          "name": "Qwen: Qwen3 Coder 480B A35B (free)",
          "created": 1753230546,
          "description": "Qwen3-Coder-480B-A35B-Instruct is a Mixture-of-Experts (MoE) code generation model developed by the Qwen team. It is optimized for agentic coding tasks such as function calling, tool use, and long-context reasoning over repositories. The model features 480 billion total parameters, with 35 billion active per forward pass (8 out of 160 experts).\n\nPricing for the Alibaba endpoints varies by context length. Once a request is greater than 128k input tokens, the higher pricing is used.",
          "architecture": {
            "input_modalities": [
              "text"
            ],
            "output_modalities": [
              "text"
            ],
            "tokenizer": "Qwen3",
            "instruct_type": null
          },
          "top_provider": {
            "is_moderated": false,
            "context_length": 262144,
            "max_completion_tokens": null
          },
          "pricing": {
            "prompt": "0",
            "completion": "0",
            "image": "0",
            "request": "0",
            "web_search": "0",
            "internal_reasoning": "0",
            "input_cache_read": "0",
            "input_cache_write": "0"
          },
          "canonical_slug": "qwen/qwen3-coder-480b-a35b-07-25",
          "context_length": 262144,
          "hugging_face_id": "Qwen/Qwen3-Coder-480B-A35B-Instruct",
          "per_request_limits": null,
          "supported_parameters": [
            "frequency_penalty",
            "logit_bias",
            "logprobs",
            "max_tokens",
            "min_p",
            "presence_penalty",
            "repetition_penalty",
            "seed",
            "stop",
            "temperature",
            "tool_choice",
            "tools",
            "top_k",
            "top_logprobs",
            "top_p"
          ]
        }
      }
    },
    {
      "id": "tngtech/deepseek-r1t2-chimera:free",
      "name": "TNG: DeepSeek R1T2 Chimera (free)",
      "provider": "openrouter",
      "description": "DeepSeek-TNG-R1T2-Chimera is the second-generation Chimera model from TNG Tech. It is a 671 B-parameter mixture-of-experts text-generation model assembled from DeepSeek-AI\u2019s R1-0528, R1, and V3-0324 checkpoints with an Assembly-of-Experts merge. The tri-parent design yields strong reasoning performance while running roughly 20 % faster than the original R1 and more than 2\u00d7 faster than R1-0528 under vLLM, giving a favorable cost-to-intelligence trade-off. The checkpoint supports contexts up to 60 k tokens in standard use (tested to ~130 k) and maintains consistent <think> token behaviour, making it suitable for long-context analysis, dialogue and other open-ended generation tasks.",
      "weight_class": "ultraweight",
      "tier": "budget",
      "context_length": 163840,
      "max_completion_tokens": 4096,
      "pricing": {
        "prompt_cost_per_1k": 0.0,
        "completion_cost_per_1k": 0.0,
        "is_free": true,
        "currency": "USD"
      },
      "value_score": 24.750000000000004,
      "is_preview": true,
      "is_text_only": true,
      "estimated_params": null,
      "source_info": {
        "openrouter_raw": {
          "id": "tngtech/deepseek-r1t2-chimera:free",
          "name": "TNG: DeepSeek R1T2 Chimera (free)",
          "created": 1751986985,
          "description": "DeepSeek-TNG-R1T2-Chimera is the second-generation Chimera model from TNG Tech. It is a 671 B-parameter mixture-of-experts text-generation model assembled from DeepSeek-AI\u2019s R1-0528, R1, and V3-0324 checkpoints with an Assembly-of-Experts merge. The tri-parent design yields strong reasoning performance while running roughly 20 % faster than the original R1 and more than 2\u00d7 faster than R1-0528 under vLLM, giving a favorable cost-to-intelligence trade-off. The checkpoint supports contexts up to 60 k tokens in standard use (tested to ~130 k) and maintains consistent <think> token behaviour, making it suitable for long-context analysis, dialogue and other open-ended generation tasks.",
          "architecture": {
            "input_modalities": [
              "text"
            ],
            "output_modalities": [
              "text"
            ],
            "tokenizer": "DeepSeek",
            "instruct_type": null
          },
          "top_provider": {
            "is_moderated": false,
            "context_length": 163840,
            "max_completion_tokens": null
          },
          "pricing": {
            "prompt": "0",
            "completion": "0",
            "image": "0",
            "request": "0",
            "web_search": "0",
            "internal_reasoning": "0",
            "input_cache_read": "0",
            "input_cache_write": "0"
          },
          "canonical_slug": "tngtech/deepseek-r1t2-chimera",
          "context_length": 163840,
          "hugging_face_id": "tngtech/DeepSeek-TNG-R1T2-Chimera",
          "per_request_limits": null,
          "supported_parameters": [
            "frequency_penalty",
            "include_reasoning",
            "logit_bias",
            "logprobs",
            "max_tokens",
            "min_p",
            "presence_penalty",
            "reasoning",
            "repetition_penalty",
            "seed",
            "stop",
            "temperature",
            "top_k",
            "top_logprobs",
            "top_p"
          ]
        }
      }
    },
    {
      "id": "deepseek/deepseek-r1-0528:free",
      "name": "DeepSeek: R1 0528 (free)",
      "provider": "openrouter",
      "description": "May 28th update to the [original DeepSeek R1](/deepseek/deepseek-r1) Performance on par with [OpenAI o1](/openai/o1), but open-sourced and with fully open reasoning tokens. It's 671B parameters in size, with 37B active in an inference pass.\n\nFully open-source model.",
      "weight_class": "ultraweight",
      "tier": "budget",
      "context_length": 163840,
      "max_completion_tokens": 4096,
      "pricing": {
        "prompt_cost_per_1k": 0.0,
        "completion_cost_per_1k": 0.0,
        "is_free": true,
        "currency": "USD"
      },
      "value_score": 24.750000000000004,
      "is_preview": true,
      "is_text_only": true,
      "estimated_params": "671B",
      "source_info": {
        "openrouter_raw": {
          "id": "deepseek/deepseek-r1-0528:free",
          "name": "DeepSeek: R1 0528 (free)",
          "created": 1748455170,
          "description": "May 28th update to the [original DeepSeek R1](/deepseek/deepseek-r1) Performance on par with [OpenAI o1](/openai/o1), but open-sourced and with fully open reasoning tokens. It's 671B parameters in size, with 37B active in an inference pass.\n\nFully open-source model.",
          "architecture": {
            "input_modalities": [
              "text"
            ],
            "output_modalities": [
              "text"
            ],
            "tokenizer": "DeepSeek",
            "instruct_type": "deepseek-r1"
          },
          "top_provider": {
            "is_moderated": false,
            "context_length": 163840,
            "max_completion_tokens": null
          },
          "pricing": {
            "prompt": "0",
            "completion": "0",
            "image": "0",
            "request": "0",
            "web_search": "0",
            "internal_reasoning": "0",
            "input_cache_read": "0",
            "input_cache_write": "0"
          },
          "canonical_slug": "deepseek/deepseek-r1-0528",
          "context_length": 163840,
          "hugging_face_id": "deepseek-ai/DeepSeek-R1-0528",
          "per_request_limits": null,
          "supported_parameters": [
            "frequency_penalty",
            "include_reasoning",
            "logit_bias",
            "logprobs",
            "max_tokens",
            "min_p",
            "presence_penalty",
            "reasoning",
            "repetition_penalty",
            "seed",
            "stop",
            "temperature",
            "top_k",
            "top_logprobs",
            "top_p"
          ]
        }
      }
    },
    {
      "id": "qwen/qwen3-235b-a22b:free",
      "name": "Qwen: Qwen3 235B A22B (free)",
      "provider": "openrouter",
      "description": "Qwen3-235B-A22B is a 235B parameter mixture-of-experts (MoE) model developed by Qwen, activating 22B parameters per forward pass. It supports seamless switching between a \"thinking\" mode for complex reasoning, math, and code tasks, and a \"non-thinking\" mode for general conversational efficiency. The model demonstrates strong reasoning ability, multilingual support (100+ languages and dialects), advanced instruction-following, and agent tool-calling capabilities. It natively handles a 32K token context window and extends up to 131K tokens using YaRN-based scaling.",
      "weight_class": "ultraweight",
      "tier": "budget",
      "context_length": 131072,
      "max_completion_tokens": 4096,
      "pricing": {
        "prompt_cost_per_1k": 0.0,
        "completion_cost_per_1k": 0.0,
        "is_free": true,
        "currency": "USD"
      },
      "value_score": 24.750000000000004,
      "is_preview": true,
      "is_text_only": true,
      "estimated_params": "235B",
      "source_info": {
        "openrouter_raw": {
          "id": "qwen/qwen3-235b-a22b:free",
          "name": "Qwen: Qwen3 235B A22B (free)",
          "created": 1745875757,
          "description": "Qwen3-235B-A22B is a 235B parameter mixture-of-experts (MoE) model developed by Qwen, activating 22B parameters per forward pass. It supports seamless switching between a \"thinking\" mode for complex reasoning, math, and code tasks, and a \"non-thinking\" mode for general conversational efficiency. The model demonstrates strong reasoning ability, multilingual support (100+ languages and dialects), advanced instruction-following, and agent tool-calling capabilities. It natively handles a 32K token context window and extends up to 131K tokens using YaRN-based scaling.",
          "architecture": {
            "input_modalities": [
              "text"
            ],
            "output_modalities": [
              "text"
            ],
            "tokenizer": "Qwen3",
            "instruct_type": "qwen3"
          },
          "top_provider": {
            "is_moderated": false,
            "context_length": 131072,
            "max_completion_tokens": null
          },
          "pricing": {
            "prompt": "0",
            "completion": "0",
            "image": "0",
            "request": "0",
            "web_search": "0",
            "internal_reasoning": "0",
            "input_cache_read": "0",
            "input_cache_write": "0"
          },
          "canonical_slug": "qwen/qwen3-235b-a22b-04-28",
          "context_length": 131072,
          "hugging_face_id": "Qwen/Qwen3-235B-A22B",
          "per_request_limits": null,
          "supported_parameters": [
            "frequency_penalty",
            "include_reasoning",
            "logit_bias",
            "logprobs",
            "max_tokens",
            "min_p",
            "presence_penalty",
            "reasoning",
            "repetition_penalty",
            "response_format",
            "seed",
            "stop",
            "structured_outputs",
            "temperature",
            "tool_choice",
            "tools",
            "top_k",
            "top_logprobs",
            "top_p"
          ]
        }
      }
    },
    {
      "id": "tngtech/deepseek-r1t-chimera:free",
      "name": "TNG: DeepSeek R1T Chimera (free)",
      "provider": "openrouter",
      "description": "DeepSeek-R1T-Chimera is created by merging DeepSeek-R1 and DeepSeek-V3 (0324), combining the reasoning capabilities of R1 with the token efficiency improvements of V3. It is based on a DeepSeek-MoE Transformer architecture and is optimized for general text generation tasks.\n\nThe model merges pretrained weights from both source models to balance performance across reasoning, efficiency, and instruction-following tasks. It is released under the MIT license and intended for research and commercial use.",
      "weight_class": "ultraweight",
      "tier": "budget",
      "context_length": 163840,
      "max_completion_tokens": 4096,
      "pricing": {
        "prompt_cost_per_1k": 0.0,
        "completion_cost_per_1k": 0.0,
        "is_free": true,
        "currency": "USD"
      },
      "value_score": 24.750000000000004,
      "is_preview": true,
      "is_text_only": true,
      "estimated_params": null,
      "source_info": {
        "openrouter_raw": {
          "id": "tngtech/deepseek-r1t-chimera:free",
          "name": "TNG: DeepSeek R1T Chimera (free)",
          "created": 1745760875,
          "description": "DeepSeek-R1T-Chimera is created by merging DeepSeek-R1 and DeepSeek-V3 (0324), combining the reasoning capabilities of R1 with the token efficiency improvements of V3. It is based on a DeepSeek-MoE Transformer architecture and is optimized for general text generation tasks.\n\nThe model merges pretrained weights from both source models to balance performance across reasoning, efficiency, and instruction-following tasks. It is released under the MIT license and intended for research and commercial use.",
          "architecture": {
            "input_modalities": [
              "text"
            ],
            "output_modalities": [
              "text"
            ],
            "tokenizer": "DeepSeek",
            "instruct_type": null
          },
          "top_provider": {
            "is_moderated": false,
            "context_length": 163840,
            "max_completion_tokens": null
          },
          "pricing": {
            "prompt": "0",
            "completion": "0",
            "image": "0",
            "request": "0",
            "web_search": "0",
            "internal_reasoning": "0",
            "input_cache_read": "0",
            "input_cache_write": "0"
          },
          "canonical_slug": "tngtech/deepseek-r1t-chimera",
          "context_length": 163840,
          "hugging_face_id": "tngtech/DeepSeek-R1T-Chimera",
          "per_request_limits": null,
          "supported_parameters": [
            "frequency_penalty",
            "include_reasoning",
            "logit_bias",
            "logprobs",
            "max_tokens",
            "min_p",
            "presence_penalty",
            "reasoning",
            "repetition_penalty",
            "seed",
            "stop",
            "temperature",
            "top_k",
            "top_logprobs",
            "top_p"
          ]
        }
      }
    },
    {
      "id": "nvidia/llama-3.1-nemotron-ultra-253b-v1:free",
      "name": "NVIDIA: Llama 3.1 Nemotron Ultra 253B v1 (free)",
      "provider": "openrouter",
      "description": "Llama-3.1-Nemotron-Ultra-253B-v1 is a large language model (LLM) optimized for advanced reasoning, human-interactive chat, retrieval-augmented generation (RAG), and tool-calling tasks. Derived from Meta\u2019s Llama-3.1-405B-Instruct, it has been significantly customized using Neural Architecture Search (NAS), resulting in enhanced efficiency, reduced memory usage, and improved inference latency. The model supports a context length of up to 128K tokens and can operate efficiently on an 8x NVIDIA H100 node.\n\nNote: you must include `detailed thinking on` in the system prompt to enable reasoning. Please see [Usage Recommendations](https://huggingface.co/nvidia/Llama-3_1-Nemotron-Ultra-253B-v1#quick-start-and-usage-recommendations) for more.",
      "weight_class": "ultraweight",
      "tier": "budget",
      "context_length": 131072,
      "max_completion_tokens": 4096,
      "pricing": {
        "prompt_cost_per_1k": 0.0,
        "completion_cost_per_1k": 0.0,
        "is_free": true,
        "currency": "USD"
      },
      "value_score": 24.750000000000004,
      "is_preview": true,
      "is_text_only": true,
      "estimated_params": "253B",
      "source_info": {
        "openrouter_raw": {
          "id": "nvidia/llama-3.1-nemotron-ultra-253b-v1:free",
          "name": "NVIDIA: Llama 3.1 Nemotron Ultra 253B v1 (free)",
          "created": 1744115059,
          "description": "Llama-3.1-Nemotron-Ultra-253B-v1 is a large language model (LLM) optimized for advanced reasoning, human-interactive chat, retrieval-augmented generation (RAG), and tool-calling tasks. Derived from Meta\u2019s Llama-3.1-405B-Instruct, it has been significantly customized using Neural Architecture Search (NAS), resulting in enhanced efficiency, reduced memory usage, and improved inference latency. The model supports a context length of up to 128K tokens and can operate efficiently on an 8x NVIDIA H100 node.\n\nNote: you must include `detailed thinking on` in the system prompt to enable reasoning. Please see [Usage Recommendations](https://huggingface.co/nvidia/Llama-3_1-Nemotron-Ultra-253B-v1#quick-start-and-usage-recommendations) for more.",
          "architecture": {
            "input_modalities": [
              "text"
            ],
            "output_modalities": [
              "text"
            ],
            "tokenizer": "Llama3",
            "instruct_type": null
          },
          "top_provider": {
            "is_moderated": false,
            "context_length": 131072,
            "max_completion_tokens": null
          },
          "pricing": {
            "prompt": "0",
            "completion": "0",
            "image": "0",
            "request": "0",
            "web_search": "0",
            "internal_reasoning": "0",
            "input_cache_read": "0",
            "input_cache_write": "0"
          },
          "canonical_slug": "nvidia/llama-3.1-nemotron-ultra-253b-v1",
          "context_length": 131072,
          "hugging_face_id": "nvidia/Llama-3_1-Nemotron-Ultra-253B-v1",
          "per_request_limits": null,
          "supported_parameters": [
            "frequency_penalty",
            "logit_bias",
            "logprobs",
            "max_tokens",
            "min_p",
            "presence_penalty",
            "repetition_penalty",
            "seed",
            "stop",
            "temperature",
            "top_k",
            "top_logprobs",
            "top_p"
          ]
        }
      }
    },
    {
      "id": "deepseek/deepseek-chat-v3-0324:free",
      "name": "DeepSeek: DeepSeek V3 0324 (free)",
      "provider": "openrouter",
      "description": "DeepSeek V3, a 685B-parameter, mixture-of-experts model, is the latest iteration of the flagship chat model family from the DeepSeek team.\n\nIt succeeds the [DeepSeek V3](/deepseek/deepseek-chat-v3) model and performs really well on a variety of tasks.",
      "weight_class": "ultraweight",
      "tier": "budget",
      "context_length": 163840,
      "max_completion_tokens": 4096,
      "pricing": {
        "prompt_cost_per_1k": 0.0,
        "completion_cost_per_1k": 0.0,
        "is_free": true,
        "currency": "USD"
      },
      "value_score": 24.750000000000004,
      "is_preview": true,
      "is_text_only": true,
      "estimated_params": "685B",
      "source_info": {
        "openrouter_raw": {
          "id": "deepseek/deepseek-chat-v3-0324:free",
          "name": "DeepSeek: DeepSeek V3 0324 (free)",
          "created": 1742824755,
          "description": "DeepSeek V3, a 685B-parameter, mixture-of-experts model, is the latest iteration of the flagship chat model family from the DeepSeek team.\n\nIt succeeds the [DeepSeek V3](/deepseek/deepseek-chat-v3) model and performs really well on a variety of tasks.",
          "architecture": {
            "input_modalities": [
              "text"
            ],
            "output_modalities": [
              "text"
            ],
            "tokenizer": "DeepSeek",
            "instruct_type": null
          },
          "top_provider": {
            "is_moderated": false,
            "context_length": 163840,
            "max_completion_tokens": null
          },
          "pricing": {
            "prompt": "0",
            "completion": "0",
            "image": "0",
            "request": "0",
            "web_search": "0",
            "internal_reasoning": "0",
            "input_cache_read": "0",
            "input_cache_write": "0"
          },
          "canonical_slug": "deepseek/deepseek-chat-v3-0324",
          "context_length": 163840,
          "hugging_face_id": "deepseek-ai/DeepSeek-V3-0324",
          "per_request_limits": null,
          "supported_parameters": [
            "frequency_penalty",
            "logit_bias",
            "logprobs",
            "max_tokens",
            "min_p",
            "presence_penalty",
            "repetition_penalty",
            "seed",
            "stop",
            "temperature",
            "tool_choice",
            "tools",
            "top_k",
            "top_logprobs",
            "top_p"
          ]
        }
      }
    }
  ],
  "timestamp": "2025-09-08T23:20:22.893926",
  "expires_at": "2025-09-09T05:20:22.893926",
  "provider": "openrouter",
  "endpoint": "models"
}